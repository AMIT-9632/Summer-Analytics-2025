{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hackathon 1\n"
      ],
      "metadata": {
        "id": "78a3ex9ROBkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i have the solution code to the first Hackathon problem of the Summer Analytics 2025 course"
      ],
      "metadata": {
        "id": "NvRoGDgJOXl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Problem Statement\n"
      ],
      "metadata": {
        "id": "dLTC3MxuOa44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hackathon Problem Statement:\n",
        "\n",
        "NDVI-based Land Cover Classification\n",
        "Key Concepts\n",
        "\n",
        "1. NDVI (Normalized Difference Vegetation Index)\n",
        "Measures vegetation health using satellite data:\n",
        "Where:-\n",
        "NIR = Near-Infrared reflectance\n",
        "Red = Red reflectance\n",
        "\n",
        "\n",
        "2. Data Challenges\n",
        "Noise: The main challenge with the dataset is that both the imagery and the crowdsourced data contain noise (due to cloud cover in the images and inaccurate labeling/digitizing of polygons).\n",
        "\n",
        "Missing Data: Certain NDVI values are missing because of cloud cover obstructing the satellite view.\n",
        "\n",
        "Temporal Variations: NDVI values vary seasonally, requiring careful feature engineering to extract meaningful trends.\n",
        "\n",
        "Important Note:\n",
        "The training and public leaderboard test data may contain noisy observations, while the private leaderboard data is clean and free of noise. This design helps evaluate how well your model generalizes beyond noisy training conditions.\n",
        "\n",
        "Dataset\n",
        "Each row in the dataset contains:\n",
        "\n",
        "class: Ground truth label of the land cover type â€” one of {Water, Impervious, Farm, Forest, Grass, Orchard}\n",
        "\n",
        "ID:Unique identifier for the sample\n",
        "\n",
        "27 NDVI Time Points: Columns labeled in the format YYYYMMDD_N (e.g., 20150720_N, 20150602_N) represent NDVI values collected on different dates. These values form a time series representing vegetation dynamics for each location.\n",
        "\n",
        "Rules\n",
        "Model: Logistic Regression only (multiclass).\n",
        "\n",
        "Preprocessing: Denoising, imputation, and feature engineering allowed.\n",
        "\n",
        "Leaderboard:\n",
        "\n",
        "Public (89% test data): Immediate feedback.\n",
        "\n",
        "Private (11% test data): Final ranking (avoids overfitting).\n",
        "\n",
        "Evaluation\n",
        "Submissions will be evaluated on basis of accuracy score of the predicted class.\n",
        "\n",
        "Submission format:\n",
        "ID,class\n",
        "1,water\n",
        "2,water\n",
        "3,grass\n",
        "4,impervious\n",
        ".."
      ],
      "metadata": {
        "id": "0H26yhtmOpm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Solution Code"
      ],
      "metadata": {
        "id": "iEpbhxBeO-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/content/hacktrain.csv')\n",
        "test = pd.read_csv('/content/hacktest.csv')\n",
        "\n",
        "# Identify NDVI columns\n",
        "ndvi_cols = [col for col in train.columns if '_N' in col]\n",
        "\n",
        "# Impute missing NDVI values (median)\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "train_ndvi = pd.DataFrame(imputer.fit_transform(train[ndvi_cols]), columns=ndvi_cols)\n",
        "test_ndvi = pd.DataFrame(imputer.transform(test[ndvi_cols]), columns=ndvi_cols)\n",
        "\n",
        "# Feature engineering\n",
        "def extract_features(df):\n",
        "    feats = pd.DataFrame()\n",
        "    feats['ndvi_mean'] = df.mean(axis=1)\n",
        "    feats['ndvi_median'] = df.median(axis=1)\n",
        "    feats['ndvi_std'] = df.std(axis=1)\n",
        "    feats['ndvi_min'] = df.min(axis=1)\n",
        "    feats['ndvi_max'] = df.max(axis=1)\n",
        "    feats['ndvi_range'] = feats['ndvi_max'] - feats['ndvi_min']\n",
        "    feats['ndvi_skew'] = df.skew(axis=1)\n",
        "    feats['ndvi_kurtosis'] = df.kurtosis(axis=1)\n",
        "    feats['ndvi_missing_prop'] = df.isna().sum(axis=1) / df.shape[1]\n",
        "    feats['ndvi_pos_count'] = (df > 0).sum(axis=1)\n",
        "    feats['ndvi_neg_count'] = (df < 0).sum(axis=1)\n",
        "    # Slope (trend)\n",
        "    time_idx = np.arange(df.shape[1])\n",
        "    feats['ndvi_slope'] = df.apply(lambda row: np.polyfit(time_idx, row, 1)[0], axis=1)\n",
        "    return feats\n",
        "\n",
        "X_train_feats = extract_features(train_ndvi)\n",
        "X_test_feats = extract_features(test_ndvi)\n",
        "\n",
        "# Optionally, concatenate raw NDVI values (if model does not overfit)\n",
        "X_train = pd.concat([train_ndvi, X_train_feats], axis=1)\n",
        "X_test = pd.concat([test_ndvi, X_test_feats], axis=1)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['class'])\n",
        "\n",
        "# Validation split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, random_state=42)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "# Validation accuracy\n",
        "val_pred = clf.predict(X_val)\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, val_pred))\n",
        "\n",
        "# Predict on test set\n",
        "test_pred = le.inverse_transform(clf.predict(X_test_scaled))\n",
        "\n",
        "# Submission\n",
        "submission = pd.DataFrame({'ID': test['ID'], 'class': test_pred})\n",
        "submission.to_csv('submission7.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr_uEODKrfGP",
        "outputId": "2f53fa77-a0be-41c3-9d2d-3c0fea81e576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.914375\n"
          ]
        }
      ]
    }
  ]
}